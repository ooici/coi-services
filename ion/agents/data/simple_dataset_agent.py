"""
TwoDelegateDatasetAgent: improved generalized implementation of external dataset agent:
- avoids artificial agent/driver communication and state model
- pluggable behavior to find new data files (Poller) and parse contents (Parser)
- hands-off interrupt/resume state mechanism: memento generated by poller only has to make sense to that poller
- RPC and database calls removed

classes defined:
- BistableDatasetAgent
    - abstract base class
    - implements two-state model (streaming or idle)
    - acts as agent, driver client, and driver; simplifies control flow
- TwoDelegateDatasetAgent
    - usable implementation takes two delegates for pluggable behavior
    - poller delegate knows how to find new data
    - parser delegate knows how to handle data
- Poller
    The poller may read files, call web services or otherwise figuring out if new datasets are available.
    When new data is found, it invokes a callback with an open file-like object and a memento.
    Data is parsed from the file-like object and published, and
    if successful the memento is saved.
    Should the agent (or container or system) crash and be restarted,
    the memento will be passed to the new poller so it knows its position
    and can resume looking for only new datasets.
- Parser
    The parser

interrupt/resume state:
- on each callback, the poller provides a memento it can use to keep its position after resume
- after successful parsing, the memento is persisted as part of the agent state
- upon restart, the agent reads the memento and passes to the new poller
"""
from ooi.logging import log
from pyon.util.containers import get_safe
from pyon.core.exception import InstDriverError
from pyon.core.exception import InstStateError
from pyon.public import OT
from pyon.core.bootstrap import IonObject
from ion.agents.instrument.exceptions import InstrumentStateException

from pyon.agent.agent import ResourceAgentEvent
from pyon.agent.agent import ResourceAgentState
from ion.agents.instrument.instrument_agent import InstrumentAgent
from ion.core.includes.mi import DriverEvent
from ooi.poller import DirectoryPoller
from pyon.agent.agent import ResourceAgentState
from pyon.ion.stream import StandaloneStreamPublisher
from ion.services.dm.utility.granule.record_dictionary import RecordDictionaryTool
from coverage_model import ParameterDictionary


class Poller(object):
    """ abstract class to show API needed for plugin poller objects """
    def __init__(self, config, memento, data_callback, exception_callback):  pass
    def start(self): pass
    def shutdown(self): pass

class Parser(object):
    """ abstract class to show API needed for plugin poller objects """
    def __init__(self, open_file, parser_after):  pass
    def get_records(self,max_count): pass


class TwoDelegateDatasetAgent(InstrumentAgent):
    """
    this dataset agent has two states: autosampling and idle
    based on InstrumentAgent but override methods to simplify flow control: agent == driver client == driver
    abstract base class, subclass is expected to override: set_configuration, start_sampling, stop_sampling

    implement functionality with two pluggable delegates:
    - poller -- waits for new datasets to be available and alerts system when found
    - parser -- reads and parses data files into record dicts
    """

    _poller = None
    _polling_exception = None

    ORIGIN_TYPE = "ExternalDataset"

    def __init__(self,*a,**b):
        super(TwoDelegateDatasetAgent,self).__init__(*a,**b)
        self._fsm.add_handler(ResourceAgentState.STREAMING, ResourceAgentEvent.EXECUTE_RESOURCE, self._handler_streaming_execute_resource)
    def _start_driver(self, dvr_config):
        try:
            self.set_configuration(dvr_config)
        except:
            log.error('error in configuration', exc_info=True)
            raise
        self._dvr_client = self
    def _stop_driver(self):
        self._dvr_client = None
    def _handler_streaming_execute_resource(self, command, *args, **kwargs):
        """
        Handler for execute_resource command in streaming state.
        Delegates to InstrumentAgent._handler_observatory_execute_resource
        """
        log.info("_handler_streaming_execute_resource")
        if command == DriverEvent.ACQUIRE_SAMPLE or command == DriverEvent.STOP_AUTOSAMPLE:
            return self._handler_execute_resource(command, *args, **kwargs)
        else:
            raise InstrumentStateException('Command \'{0}\' not allowed in current state {1}'.format(command, self._fsm.get_current_state()))
    def _handler_active_unknown_go_inactive(self, *args, **kwargs):
        self.stop_sampling()
        return (ResourceAgentState.INACTIVE, None)
    def _handler_inactive_go_active(self, *args, **kwargs):
        self.start_sampling()
        return (ResourceAgentState.IDLE, None)
    def cmd_dvr(self, cmd, *args, **kwargs):
        if cmd == 'execute_start_autosample':
            # Delegate to BaseDataHandler.execute_start_autosample()
            self.start_sampling()
            return (None, None)
        elif cmd == 'execute_stop_autosample':
            self.stop_sampling()
            return (ResourceAgentState.IDLE, None)
        elif cmd == 'execute_resource':
            log.warn("cmd_dvr.execute_resource %r %r",args,kwargs)
            return (None,None)

    def _validate_driver_config(self):
        out = True
        for key in 'stream_id', 'stream_route', 'poller.module', 'poller.class', 'parser.module', 'parser.class', 'parameter_dict':
            if key not in self._dvr_config:
                log.error('missing key: %s',key)
                out = False
        if get_safe(self._dvr_config, 'max_records', 100)<1:
            log.error('max_rate=%d, must be at least 1', self.max_records)
            out = False
        return out

    def set_configuration(self, config):
        """
        expect configuration to have:
        - parser module/class
        - directory, wildcard to find data files
        - optional timestamp of last granule
        - optional poll rate
        - publish info
        """
        log.debug('using configuration: %s', config)
        try:
            self.config = config
            self.max_records = get_safe(config, 'max_records', 100)
            stream_id = config['stream_id']
            stream_route_param = config['stream_route']
            stream_route = IonObject(OT.StreamRoute, stream_route_param)
            self.publisher = StandaloneStreamPublisher(stream_id=stream_id, stream_route=stream_route)

            poller_module_name = config['poller.module']
            poller_class_name = config['poller.class']
            poller_module = __import__(poller_module_name, fromlist=[poller_class_name])
            self.poller_class = getattr(poller_module, poller_class_name)

            parser_module_name = config['parser.module']
            parser_class_name = config['parser.class']
            parser_module = __import__(parser_module_name, fromlist=[parser_class_name])
            self.parser_class = getattr(parser_module, parser_class_name)
            self.parameter_dictionary = ParameterDictionary.load(config['parameter_dict'])
            self.time_field = self.parameter_dictionary.get_temporal_context()
            self.latest_granule_time = get_safe(config, 'last_time', 0)
        except:
            log.error('problem in agent configuration', exc_info=True)
            raise
    def start_sampling(self):
        log.debug('start_sampling')
        if self._poller:
            log.error('already polling')
            raise InstStateError('already polling')
        try:
            memento = self._get_state('poller_state')
            self._poller = self.poller_class(self.config, memento, self.poller_callback, self.exception_callback)
            self._poller.start()
        except:
            log.error('exception starting poller', exc_info=True)
            raise
    def poller_callback(self, file_like_object, state_memento):
        log.debug('poller found data to parse')
        try:
            parser = self.parser_class(open_file=file_like_object, parse_after=self.latest_granule_time)
            records = parser.get_records(max_count=self.max_records)
            log.trace('have %d records', len(records))
            while records:
                # secretly uses pubsub client
                rdt = RecordDictionaryTool(param_dictionary=self.parameter_dictionary)
                for key in records[0]: #assume all dict records have same keys
                    rdt[key] = [ record[key] for record in records ]
                g = rdt.to_granule()
                self.publisher.publish(g)
                records = parser.get_records(max_count=self.max_records)
            self._set_state('poller_state', state_memento)
        except:
            log.error('error handling data', exc_info=True)

    def exception_callback(self, exception):
        log.error('error in poller', exc_info=True)
        self.stop_sampling()

    def stop_sampling(self):
        log.debug('stop_sampling')
        self._poller.shutdown()

## once we want to support more than one way to reach external datasets,
## we'll probably want to move this out into another file and add
## other pollers that check HTTP, FTP or other methods of finding data

class AdditiveSequentialFilePoller(DirectoryPoller,Poller):
    """ polls directory for files that match wildcard, in order """
    def __init__(self, config, memento, file_callback, exception_callback):
        self.callback = file_callback
        self.last_file_completed = memento
        DirectoryPoller.__init__(self, config['directory'], config['pattern'], self.on_new_files,
                                 exception_callback, get_safe(config, 'frequency', 300))
    def on_new_files(self, files):
        for file in files:
            if file>self.last_file_completed:
                with open(file,'rb') as f:
                    self.callback(f, file)
