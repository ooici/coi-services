#!/usr/bin/env python

"""
TwoDelegateDatasetAgent: improved generalized implementation of external dataset agent:
- avoids artificial agent/driver communication and state model
- pluggable behavior to find new data files (Poller) and parse contents (Parser)
- hands-off interrupt/resume state mechanism: memento generated by poller only has to make sense to that poller
- RPC and database calls removed

classes defined:
- BistableDatasetAgent
    - abstract base class
    - implements two-state model (streaming or idle)
    - acts as agent, driver client, and driver; simplifies control flow
- TwoDelegateDatasetAgent
    - usable implementation takes two delegates for pluggable behavior
    - poller delegate knows how to find new data
    - parser delegate knows how to handle data
- Poller
    The poller may read files, call web services or otherwise figuring out if new datasets are available.
    When new data is found, it invokes a callback with an open file-like object and a memento.
    Data is parsed from the file-like object and published, and
    if successful the memento is saved.
    Should the agent (or container or system) crash and be restarted,
    the memento will be passed to the new poller so it knows its position
    and can resume looking for only new datasets.
- Parser
    The parser

interrupt/resume state:
- on each callback, the poller provides a memento it can use to keep its position after resume
- after successful parsing, the memento is persisted as part of the agent state
- upon restart, the agent reads the memento and passes to the new poller
"""

__author__ = 'Christopher Mueller, Jonathan Newbrough'


import os

from ooi.logging import log
from ooi.reflection import EggCache
from ooi.poller import DirectoryPoller

from pyon.agent.agent import ResourceAgentEvent
from pyon.agent.agent import ResourceAgentState
from pyon.core.exception import InstStateError
from pyon.public import OT
from pyon.core.bootstrap import IonObject
from pyon.util.containers import get_safe
from pyon.ion.stream import StandaloneStreamPublisher

from ion.agents.instrument.exceptions import InstrumentStateException
from ion.agents.instrument.instrument_agent import InstrumentAgent
from ion.core.includes.mi import DriverEvent
from ion.services.dm.utility.granule.record_dictionary import RecordDictionaryTool

from coverage_model import ParameterDictionary

# TODO: make unique for multiple processes on same VM
EGG_CACHE=EggCache('/tmp/eggs%d' % os.getpid())


class Poller(object):
    """ abstract class to show API needed for plugin poller objects """
    def __init__(self, config, memento, data_callback, exception_callback):  
        pass
    
    def start(self): 
        pass
    
    def shutdown(self): 
        pass


class Parser(object):
    """ abstract class to show API needed for plugin poller objects """
    def __init__(self, open_file, parser_after):  
        pass
    
    def get_records(self, max_count):
        """
        Returns a list of particles (following the instrument driver structure).
        """
        pass


class TwoDelegateDatasetAgent(InstrumentAgent):
    """
    this dataset agent has two states: autosampling and idle
    based on InstrumentAgent but override methods to simplify flow control: agent == driver client == driver
    abstract base class, subclass is expected to override: set_configuration, start_sampling, stop_sampling

    implement functionality with two pluggable delegates:
    - poller -- waits for new datasets to be available and alerts system when found
    - parser -- reads and parses data files into record dicts
    """

    _poller = None
    _polling_exception = None

    ORIGIN_TYPE = "ExternalDataset"

    def __init__(self, *args, **kwargs):
        super(TwoDelegateDatasetAgent,self).__init__(*args, **kwargs)
        self._fsm.add_handler(ResourceAgentState.STREAMING, ResourceAgentEvent.EXECUTE_RESOURCE, self._handler_streaming_execute_resource)
        log.warn("DRIVER: __init__")

    def _start_driver(self, dvr_config):
        log.warn("DRIVER: _start_driver: %s", dvr_config)
        try:
            self.set_configuration(dvr_config)
        except:
            log.error('error in configuration', exc_info=True)
            raise
        self._dvr_client = self
        self._asp.reset_connection()

    def _stop_driver(self):
        self.stop_sampling()
        self._dvr_client = None

    def _handler_streaming_execute_resource(self, command, *args, **kwargs):
        """
        Handler for execute_resource command in streaming state.
        Delegates to InstrumentAgent._handler_observatory_execute_resource
        """
        if command == DriverEvent.ACQUIRE_SAMPLE or command == DriverEvent.STOP_AUTOSAMPLE:
            return self._handler_execute_resource(command, *args, **kwargs)
        else:
            raise InstrumentStateException('Command \'{0}\' not allowed in current state {1}'.format(command, self._fsm.get_current_state()))

    def _handler_active_unknown_go_inactive(self, *args, **kwargs):
        self.stop_sampling()
        return (ResourceAgentState.INACTIVE, None)

    def _handler_inactive_go_active(self, *args, **kwargs):
        self.start_sampling()
        return (ResourceAgentState.IDLE, None)

    def cmd_dvr(self, cmd, *args, **kwargs):
        log.warn("DRIVER: cmd_dvr %s", cmd)
        if cmd == 'execute_start_autosample':
            # Delegate to BaseDataHandler.execute_start_autosample()
            self.start_sampling()
            return (None, None)
        elif cmd == 'execute_stop_autosample':
            self.stop_sampling()
            return (ResourceAgentState.IDLE, None)
        elif cmd == 'execute_resource':
            log.warn("cmd_dvr.execute_resource %r %r",args,kwargs)
            return (None,None)

    def _validate_driver_config(self):
        out = True
        for key in 'poller', 'parser':
            if key not in self._dvr_config:
                log.error('missing key: %s', key)
                out = False
        for key in ('stream_config', ):
            if key not in self.CFG:
                log.error('missing key: %s', key)
                out = False
        if get_safe(self._dvr_config, 'max_records', 100) < 1:
            log.error('max_records=%d, must be at least 1 or unset (default 100)', self.max_records)
            out = False
        return out

    def set_configuration(self, config):
        log.warn("DRIVER: set_configuration")
        """
        expect configuration to have:
        - parser module/class
        - directory, wildcard to find data files
        - optional timestamp of last granule
        - optional poll rate
        - publish info
        """
        log.error("Log level: %s", log.getEffectiveLevel())
        log.debug('using configuration: %s', config)
        self.config = config
        self.max_records = get_safe(config, 'max_records', 100)
        self.stream_config = self.CFG.get('stream_config', {})
        if len(self.stream_config) == 1:
            stream_cfg = self.stream_config.values()[0]
        elif len(self.stream_config) > 1:
            stream_cfg = self.stream_config.values()[0]

        stream_id = stream_cfg['stream_id']
        stream_route = IonObject(OT.StreamRoute, routing_key=stream_cfg['routing_key'], exchange_point=stream_cfg['exchange_point'])
        param_dict = stream_cfg['stream_def_dict']['parameter_dictionary']
        self.publisher = StandaloneStreamPublisher(stream_id=stream_id, stream_route=stream_route)
        self.parameter_dictionary = ParameterDictionary.load(param_dict)
        self.time_field = self.parameter_dictionary.get_temporal_context()
        self.latest_granule_time = get_safe(config, 'last_time', 0)

    def _create_plugin(self, config, args=None, kwargs=None):
        args, kwargs = args or [], kwargs or {}
        uri = config['uri']
        egg_name = uri.split('/')[-1] if uri.startswith('http') else uri
        egg_repo = uri[0:len(uri)-len(egg_name)-1] if uri.startswith('http') else None
        module_name = config['module']
        class_name = config['class']
        return EGG_CACHE.get_object(class_name, module_name, egg_name, egg_repo, args, kwargs)

    def start_sampling(self):
        if self._poller:
            raise InstStateError('already polling')
        memento = self._get_state('poller_state')
        config = self.config['poller']
        log.trace('poller config: %r', config)
        self._poller = self._create_plugin(config, args=[config['config'], memento, self.poller_callback, self.exception_callback])
        self._poller.start()

    def poller_callback(self, file_like_object, state_memento):
        log.debug('poller found data to parse')
        try:
            config = self.config['parser']
            parser = self._create_plugin(config, kwargs=dict(open_file=file_like_object, parse_after=self.latest_granule_time))
            records = parser.get_records(max_count=self.max_records)
            log.trace('have %d records', len(records))
            while records:
                self._asp.on_sample_mult(records)
                # # secretly uses pubsub client
                # rdt = RecordDictionaryTool(param_dictionary=self.parameter_dictionary)
                # for key in records[0]: #assume all dict records have same keys
                #     rdt[key] = [ record[key] for record in records ]
                # g = rdt.to_granule()
                # self.publisher.publish(g)
                records = parser.get_records(max_count=self.max_records)
            self._set_state('poller_state', state_memento)
        except Exception as ex:
            log.error('error handling data', exc_info=True)

    def exception_callback(self, exception):
        log.error('error in poller', exc_info=True)
        self.stop_sampling()

    def stop_sampling(self):
        log.debug('stop_sampling')
        self._poller.shutdown()


## once we want to support more than one way to reach external datasets,
## we'll probably want to move this out into another file and add
## other pollers that check HTTP, FTP or other methods of finding data

class AdditiveSequentialFilePoller(DirectoryPoller, Poller):
    """ polls directory for files that match wildcard, in order """
    def __init__(self, config, memento, file_callback, exception_callback):
        self.callback = file_callback
        self.last_file_completed = memento
        DirectoryPoller.__init__(self, config['directory'], config['pattern'], self.on_new_files,
                                 exception_callback, get_safe(config, 'frequency', 300))
    def on_new_files(self, files):
        for file in files:
            if file>self.last_file_completed:
                with open(file,'rb') as f:
                    self.callback(f, file)
